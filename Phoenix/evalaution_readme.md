
This notebook performs structured evaluation of tool-usage predictions generated by an LLM. It supports both:

- âœ… **Code-based evaluation** (for executable tools like Python code or visualizations)
- ğŸ§  **LLM-as-a-judge** evaluation (for router decisions and tool invocation structure)

---

## ğŸ“ File Overview

- `evaluation.ipynb` â€” Main evaluation notebook
- `utils.py` â€” Contains constants like dataset paths *(assumed from context)*
- `data/` â€” Directory for input files like CSVs or JSONs for tool output evaluation

---

## ğŸ§© Evaluation Sections

### ğŸ”€ Router Evaluations (LLM-as-a-Judge)

This section checks whether the **right tool was selected** by the LLM for a given user query. Evaluation is done using:

- Prompt template (e.g., `TOOL_CALLING_PROMPT_TEMPLATE`)
- OpenAIâ€™s `gpt-4` model (or configurable model)
- Phoenixâ€™s `llm_classify()` method for structured evaluation

**Use Case:** Choosing between tools like search, plot, compute, or summarize.

---

### ğŸ› ï¸ Tool Calling Evaluations (LLM-as-a-Judge)

Evaluates whether the **structure of tool calls** made by the LLM is correct.

- Compares the predicted tool JSON/function call with the reference
- Uses LLM to determine whether the prediction is semantically and syntactically valid

**Example:** Checking if the predicted tool call contains the right arguments and intent.

---

### ğŸ§ª Code-Based Evaluations (Executable Tools)

This section executes and evaluates **code generated by the LLM**, especially for tools like:

- Python-based data visualization
- Data manipulation
- Simple computation

**How it works:**

1. Code snippets are executed (e.g., using `exec()`).
2. The actual result is compared with expected output (e.g., plots, data summaries).
3. Errors, exceptions, or mismatches are logged.

**Advantage:** More objective than LLM-based evaluations for code generation.

---

## âš™ï¸ How to Run

1. Install dependencies:

```bash
pip install phoenix openai tqdm nest_asyncio matplotlib pandas
```

2. Open `evaluation.ipynb` in Jupyter.
3. Update model name, file paths, or prompt templates if needed.
4. Run cells section-by-section or use `Run All`.

---

## ğŸ› ï¸ Under the Hood

- **Phoenix Tracing & Evaluation DSL:** Used to log and evaluate spans of tool usage
- **OpenAI LLMs:** Used as a judge for subjective evaluations
- **TQDM:** Used to track evaluation progress
- **Code Execution:** Wrapped in `try/except` blocks for safety

---

## âœ… TODO

- [ ] Integrate visualization of evaluation results (e.g., success/failure rates)
- [ ] Enable result logging to file
- [ ] Add notebook widgets for easy model and file selection

---

## ğŸ“¬ Feedback & Contributions

Feel free to open issues or PRs if you'd like to contribute or suggest improvements.
