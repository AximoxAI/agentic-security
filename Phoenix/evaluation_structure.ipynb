{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b96077cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from phoenix.evals import OpenAIModel , llm_classify , TOOL_CALLING_PROMPT_TEMPLATE\n",
    "from phoenix.experiments import run_experiment , evaluate_experiment\n",
    "from phoenix.experiments.types import Example\n",
    "from phoenix.experiments.evaluators import create_evaluator\n",
    "from phoenix.otel import register\n",
    "import pandas as pd\n",
    "from utils3 import run_agent , tools , process_messages , update_sql_gen_prompt\n",
    "from helper import get_phoenix_endpoint\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import os \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fab24adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_classify |          | 0/1 (0.0%) | ‚è≥ 05:00<? | ?it/s\n"
     ]
    }
   ],
   "source": [
    "eval_model = OpenAIModel(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96e6c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px_client = px.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28330244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading dataset...\n",
      "üíæ Examples uploaded: http://127.0.0.1:6006/datasets/RGF0YXNldDox/examples\n",
      "üóÑÔ∏è Dataset version ID: RGF0YXNldFZlcnNpb246MQ==\n"
     ]
    }
   ],
   "source": [
    "overall_experiment_questions = [\n",
    "    {'question': 'What was the most popular product SKU?',\n",
    "     'sql_result': '   SKU_Coded  Total_Qty_Sold 0    6200700         52262.0', \n",
    "     'sql_generated': '```sql\\nSELECT SKU_Coded, SUM(Qty_Sold) AS Total_Qty_Sold\\nFROM sales\\nGROUP BY SKU_Coded\\nORDER BY Total_Qty_Sold DESC\\nLIMIT 1;\\n```'},\n",
    "    {'question': 'What was the total revenue across all stores?', \n",
    "     'sql_result': '   Total_Revenue 0   1.327264e+07', \n",
    "     'sql_generated': '```sql\\nSELECT SUM(Total_Sale_Value) AS Total_Revenue\\nFROM sales;\\n```'},\n",
    "    {'question': 'Which store had the highest sales volume?',\n",
    "     'sql_result': '   Store_Number  Total_Sales_Volume 0          2970             59322.0', \n",
    "     'sql_generated': '```sql\\nSELECT Store_Number, SUM(Total_Sale_Value) AS Total_Sales_Volume\\nFROM sales\\nGROUP BY Store_Number\\nORDER BY Total_Sales_Volume DESC\\nLIMIT 1;\\n```'},\n",
    "    {'question': 'Create a bar chart showing total sales by store',\n",
    "     'sql_result': '    Store_Number    Total_Sales 0            880  420302.088397 1           1650  580443.007953 2           4180  272208.118542 3            550  229727.498752 4           1100  497509.528013 5           3300  619660.167018 6           3190  335035.018792 7           2970  836341.327191 8           3740  359729.808228 9           2530  324046.518720 10          4400   95745.620250 11          1210  508393.767785 12           330  370503.687331 13          2750  453664.808068 14          1980  242290.828499 15          1760  350747.617798 16          3410  410567.848126 17           990  378433.018639 18          4730  239711.708869 19          4070  322307.968330 20          3080  495458.238811 21          2090  309996.247965 22          1320  592832.067579 23          2640  308990.318559 24          1540  427777.427815 25          4840  389056.668316 26          2860  132320.519487 27          2420  406715.767402 28           770  292968.918642 29          3520  145701.079372 30           660  343594.978075 31          3630  405034.547846 32          2310  412579.388504 33          2200  361173.288199 34          1870  401070.997685', \n",
    "     'sql_generated': '```sql\\nSELECT Store_Number, SUM(Total_Sale_Value) AS Total_Sales\\nFROM sales\\nGROUP BY Store_Number;\\n```'},\n",
    "    {'question': 'What percentage of items were sold on promotion?',\n",
    "     'sql_result': '   Promotion_Percentage 0              0.625596',\n",
    "     'sql_generated': \"```sql\\nSELECT \\n    (SUM(CASE WHEN On_Promo = 'Yes' THEN 1 ELSE 0 END) * 100.0) / COUNT(*) AS Promotion_Percentage\\nFROM \\n    sales;\\n```\"},\n",
    "    {'question': 'What was the average transaction value?',\n",
    "     'sql_result': '   Average_Transaction_Value 0                  19.018132',\n",
    "     'sql_generated': '```sql\\nSELECT AVG(Total_Sale_Value) AS Average_Transaction_Value\\nFROM sales;\\n```'},\n",
    "    {'question': 'Create a line chart showing sales in 2021',\n",
    "     'sql_result': '  sale_month  total_quantity_sold  total_sales_value 0 2021-11-01              43056.0      499984.428193 1 2021-12-01              75724.0      910982.118423', \n",
    "     'sql_generated': '```sql\\nSELECT MONTH(Sold_Date) AS Month, SUM(Total_Sale_Value) AS Total_Sales\\nFROM sales\\nWHERE YEAR(Sold_Date) = 2021\\nGROUP BY MONTH(Sold_Date)\\nORDER BY MONTH(Sold_Date);\\n```'}\n",
    "]\n",
    "overall_experiment_df= pd.DataFrame(overall_experiment_questions)\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "dataset = px_client.upload_dataset(dataframe=overall_experiment_df, dataset_name=f\"overall_experiment_inputs-{now}\" , input_keys=[\"question\"], output_keys=[\"sql_result\" ,\"sql_generated\" ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f611725",
   "metadata": {},
   "source": [
    "Setting up the Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7048e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLARITY_LLM_JUDGE_PROMPT = \"\"\"\n",
    "In this task, you will be presented with a query and an answer. Your objective is to evaluate the clarity \n",
    "of the answer in addressing the query. A clear response is one that is precise, coherent, and directly \n",
    "addresses the query without introducing unnecessary complexity or ambiguity. An unclear response is one \n",
    "that is vague, disorganized, or difficult to understand, even if it may be factually correct.\n",
    "\n",
    "Your response should be a single word: either \"clear\" or \"unclear,\" and it should not include any other \n",
    "text or characters. \"clear\" indicates that the answer is well-structured, easy to understand, and \n",
    "appropriately addresses the query. \"unclear\" indicates that the answer is ambiguous, poorly organized, or \n",
    "not effectively communicated. Please carefully consider the query and answer before determining your \n",
    "response.\n",
    "\n",
    "After analyzing the query and the answer, you must write a detailed explanation of your reasoning to \n",
    "justify why you chose either \"clear\" or \"unclear.\" Avoid stating the final label at the beginning of your \n",
    "explanation. Your reasoning should include specific points about how the answer does or does not meet the \n",
    "criteria for clarity.\n",
    "\n",
    "[BEGIN DATA]\n",
    "Query: {query}\n",
    "Answer: {response}\n",
    "[END DATA]\n",
    "Please analyze the data carefully and provide an explanation followed by your response.\n",
    "\n",
    "EXPLANATION: Provide your reasoning step by step, evaluating the clarity of the answer based on the query.\n",
    "LABEL: \"clear\" or \"unclear\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a3e9e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_CORRECTNESS_LLM_JUDGE_PROMPT = \"\"\"\n",
    "In this task, you will be presented with a query and an answer. Your objective is to determine whether all \n",
    "the entities mentioned in the answer are correctly identified and accurately match those in the query. An \n",
    "entity refers to any specific person, place, organization, date, or other proper noun. Your evaluation \n",
    "should focus on whether the entities in the answer are correctly named and appropriately associated with \n",
    "the context in the query.\n",
    "\n",
    "Your response should be a single word: either \"correct\" or \"incorrect,\" and it should not include any \n",
    "other text or characters. \"correct\" indicates that all entities mentioned in the answer match those in the \n",
    "query and are properly identified. \"incorrect\" indicates that the answer contains errors or mismatches in \n",
    "the entities referenced compared to the query.\n",
    "\n",
    "After analyzing the query and the answer, you must write a detailed explanation of your reasoning to \n",
    "justify why you chose either \"correct\" or \"incorrect.\" Avoid stating the final label at the beginning of \n",
    "your explanation. Your reasoning should include specific points about how the entities in the answer do or \n",
    "do not match the entities in the query.\n",
    "\n",
    "[BEGIN DATA]\n",
    "Query: {query}\n",
    "Answer: {response}\n",
    "[END DATA]\n",
    "Please analyze the data carefully and provide an explanation followed by your response.\n",
    "\n",
    "EXPLANATION: Provide your reasoning step by step, evaluating whether the entities in the answer are \n",
    "correct and consistent with the query.\n",
    "LABEL: \"correct\" or \"incorrect\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfcbb0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator for the router\n",
    "def function_calling_eval(input:str,output:str) ->float:\n",
    "    if output is None:\n",
    "        return 0\n",
    "    function_calls = output.get(\"tool_calls\")\n",
    "    if function_calls:\n",
    "        eval_df =pd.DataFrame({\n",
    "            \"question\":[input.get(\"question\")]*len(function_calls),\n",
    "            \"tool_call\":function_calls\n",
    "        })\n",
    "\n",
    "        tool_call_eval = llm_classify(\n",
    "            data = eval_df,\n",
    "            template = TOOL_CALLING_PROMPT_TEMPLATE.template[0].template.replace(\"{tool_definitions}\",json.dumps(tools).replace(\"{\" ,'\"').replace(\"}\" ,'\"')),\n",
    "            rails = ['correct' , 'incorrect'],\n",
    "            model = eval_model,\n",
    "            provide_explanation = True\n",
    "        )\n",
    "\n",
    "        tool_call_eval['score'] = tool_call_eval.apply(lambda x: 1 if x['label']=='correct' else 0, axis =1)\n",
    "        return tool_call_eval['score'].mean()\n",
    "    else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "756b847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sql_result(output , expected)-> bool:\n",
    "    if output is None:\n",
    "        return False\n",
    "    sql_result = output.get(\"tool_response\")\n",
    "    if not sql_result:\n",
    "        return True\n",
    "    \n",
    "    #find first lookup sales data response\n",
    "    sql_result = next((r for r in sql_result if r.get(\"tool_name\")==\"lookup_sales_data\"), None)\n",
    "    if not sql_result:\n",
    "        return True\n",
    "    \n",
    "    #get the first response\n",
    "    sql_result = sql_result.get(\"tool_response\",\"\")\n",
    "\n",
    "    #extract just the numbers from noth strings\n",
    "    result_nums = ''.join(filter(str.isdigit,sql_result))\n",
    "    expected_nums = ''.join(filter(str.isdigit,expected.get(\"sql_result\")))\n",
    "    return result_nums == expected_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89ba582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator for tool 2: data analysis\n",
    "def evaluate_clarity(output: str, input: str) -> bool:\n",
    "    if output is None:\n",
    "        return False\n",
    "    df = pd.DataFrame({\"query\": [input.get(\"question\")],\n",
    "                       \"response\": [output.get(\"final_output\")]})\n",
    "    response = llm_classify(\n",
    "        data=df,\n",
    "        template=CLARITY_LLM_JUDGE_PROMPT,\n",
    "        rails=[\"clear\", \"unclear\"],\n",
    "        model=eval_model,\n",
    "        provide_explanation=True\n",
    "    )\n",
    "    return response['label'] == 'clear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d3c209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator for tool 2: data analysis\n",
    "def evaluate_entity_correctness(output: str, input: str) -> bool:\n",
    "    if output is None:\n",
    "        return False\n",
    "    df = pd.DataFrame({\"query\": [input.get(\"question\")], \n",
    "                       \"response\": [output.get(\"final_output\")]})\n",
    "    response = llm_classify(\n",
    "        data=df,\n",
    "        template=ENTITY_CORRECTNESS_LLM_JUDGE_PROMPT,\n",
    "        rails=[\"correct\", \"incorrect\"],\n",
    "        model=eval_model,\n",
    "        provide_explanation=True\n",
    "    )\n",
    "    return response['label'] == 'correct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c015f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator for tool 3: data visualization   \n",
    "def code_is_runnable(output: str) -> bool:\n",
    "    \"\"\"Check if the code is runnable\"\"\"\n",
    "    if output is None:\n",
    "        return False\n",
    "    generated_code = output.get(\"tool_responses\")\n",
    "    if not generated_code:\n",
    "        return True\n",
    "    \n",
    "    # Find first lookup_sales_data response\n",
    "    generated_code = next((r for r in generated_code if r.get(\"tool_name\") == \"generate_visualization\"), None)\n",
    "    if not generated_code:\n",
    "        return True\n",
    "        \n",
    "    # Get the first response\n",
    "    generated_code = generated_code.get(\"tool_response\", \"\")\n",
    "    generated_code = generated_code.strip()\n",
    "    generated_code = generated_code.replace(\"```python\", \"\").replace(\"```\", \"\")\n",
    "    try:\n",
    "        exec(generated_code)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e5290",
   "metadata": {},
   "source": [
    "defining the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd9a90b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_task(example: Example) -> str:\n",
    "    print(\"Starting agent with messages:\", example.input.get(\"question\"))\n",
    "    messages = [{\"role\": \"user\", \"content\": example.input.get(\"question\")}]\n",
    "    ret = run_agent(messages)\n",
    "    return process_messages(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d89fde",
   "metadata": {},
   "source": [
    "running the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5cfe1314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: http://127.0.0.1:6006/datasets/RGF0YXNldDox/experiments\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDox/compare?experimentId=RXhwZXJpbWVudDox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting agent with messages: What was the most popular product SKU?\n",
      "Running agent with messages: [{'role': 'user', 'content': 'What was the most popular product SKU?'}]\n",
      "Added system prompt to messages\n",
      "Starting router\n",
      "Received response with tool calls: True\n",
      "Processing tool calls\n",
      "Starting router\n",
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n",
      "Starting agent with messages: What was the total revenue across all stores?\n",
      "Running agent with messages: [{'role': 'user', 'content': 'What was the total revenue across all stores?'}]\n",
      "Added system prompt to messages\n",
      "Starting router\n",
      "Received response with tool calls: True\n",
      "Processing tool calls\n",
      "Starting router\n",
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n",
      "Starting agent with messages: Which store had the highest sales volume?\n",
      "Running agent with messages: [{'role': 'user', 'content': 'Which store had the highest sales volume?'}]\n",
      "Added system prompt to messages\n",
      "Starting router\n",
      "Received response with tool calls: True\n",
      "Processing tool calls\n",
      "Starting router\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n",
      "Starting agent with messages: Create a bar chart showing total sales by store\n",
      "Running agent with messages: [{'role': 'user', 'content': 'Create a bar chart showing total sales by store'}]\n",
      "Added system prompt to messages\n",
      "Starting router\n",
      "Received response with tool calls: True\n",
      "Processing tool calls\n",
      "Starting router\n",
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n",
      "Starting agent with messages: What percentage of items were sold on promotion?\n",
      "Running agent with messages: [{'role': 'user', 'content': 'What percentage of items were sold on promotion?'}]\n",
      "Added system prompt to messages\n",
      "Starting router\n",
      "Received response with tool calls: True\n",
      "Processing tool calls\n",
      "Starting router\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n",
      "Starting agent with messages: What was the average transaction value?\n",
      "Running agent with messages: [{'role': 'user', 'content': 'What was the average transaction value?'}]\n",
      "Added system prompt to messages\n",
      "Starting router\n",
      "Received response with tool calls: True\n",
      "Processing tool calls\n",
      "Starting router\n",
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n",
      "Starting agent with messages: Create a line chart showing sales in 2021\n",
      "Running agent with messages: [{'role': 'user', 'content': 'Create a line chart showing sales in 2021'}]\n",
      "Added system prompt to messages\n",
      "Starting router\n",
      "Received response with tool calls: True\n",
      "Processing tool calls\n",
      "Starting router\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 (100.0%) | ‚è≥ 00:24<00:00 |  3.43s/it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.54s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.78s/it\n",
      "/Users/priyanka./Documents/ai-agents/venv/lib/python3.11/site-packages/phoenix/experiments/evaluators/utils.py:229: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  return EvaluationResult(score=float(result))\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:05<00:00 |  5.93s/it\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.20s/it\n",
      "/Users/priyanka./Documents/ai-agents/venv/lib/python3.11/site-packages/phoenix/experiments/evaluators/utils.py:229: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  return EvaluationResult(score=float(result))\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:04<00:00 |  4.48s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:05<00:00 |  5.25s/it\n",
      "/Users/priyanka./Documents/ai-agents/venv/lib/python3.11/site-packages/phoenix/experiments/evaluators/utils.py:229: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  return EvaluationResult(score=float(result))\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:06<00:00 |  6.81s/it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:04<00:00 |  4.33s/it\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:07<00:00 |  7.10s/it\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.72s/it\n",
      "/Users/priyanka./Documents/ai-agents/venv/lib/python3.11/site-packages/phoenix/experiments/evaluators/utils.py:229: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  return EvaluationResult(score=float(result))\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.69s/it\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:04<00:00 |  4.17s/it\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:06<00:00 |  6.40s/it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:07<00:00 |  7.59s/it\n",
      "/Users/priyanka./Documents/ai-agents/venv/lib/python3.11/site-packages/phoenix/experiments/evaluators/utils.py:229: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  return EvaluationResult(score=float(result))\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.97s/it\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:04<00:00 |  4.51s/it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:08<00:00 |  8.82s/it\n",
      "/Users/priyanka./Documents/ai-agents/venv/lib/python3.11/site-packages/phoenix/experiments/evaluators/utils.py:229: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  return EvaluationResult(score=float(result))\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.82s/it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:08<00:00 |  8.54s/it\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:09<00:00 |  9.84s/it\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:04<00:00 |  4.42s/it\n",
      "/Users/priyanka./Documents/ai-agents/venv/lib/python3.11/site-packages/phoenix/experiments/evaluators/utils.py:229: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  return EvaluationResult(score=float(result))\n",
      "\n",
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 (100.0%) | ‚è≥ 00:55<00:00 |  1.58s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDox/compare?experimentId=RXhwZXJpbWVudDox\n",
      "\n",
      "Experiment Summary (07/10/25 08:17 PM +0530)\n",
      "--------------------------------------------\n",
      "                     evaluator  n  n_scores  avg_score  n_labels top_2_labels\n",
      "0             code_is_runnable  7         7   1.000000         7  {'True': 7}\n",
      "1             evaluate_clarity  7         7   0.285714         0         None\n",
      "2  evaluate_entity_correctness  7         7   0.857143         0         None\n",
      "3          evaluate_sql_result  7         7   1.000000         7  {'True': 7}\n",
      "4        function_calling_eval  7         7   0.000000         0         None\n",
      "\n",
      "Tasks Summary (07/10/25 08:16 PM +0530)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0           7       7         0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = run_experiment(dataset,\n",
    "                            run_agent_task,\n",
    "                            evaluators=[function_calling_eval,\n",
    "                                        evaluate_sql_result, \n",
    "                                        evaluate_clarity, \n",
    "                                        evaluate_entity_correctness, \n",
    "                                        code_is_runnable],\n",
    "                            experiment_name=\"Overall Experiment\",\n",
    "                            experiment_description=\"Evaluating the overall experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9e45a1",
   "metadata": {},
   "source": [
    "running the experiment - change in prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6610066",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = \"\"\"\n",
    "Generate an SQL query based on a prompt. \n",
    "Do not reply with anything besides the SQL query.\n",
    "The prompt is: {prompt}\n",
    "\n",
    "The available columns are: {columns}\n",
    "The table name is: {table_name}\n",
    "\n",
    "Think before you respond.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "560b12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_sql_gen_prompt(new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6189304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: http://127.0.0.1:6006/datasets/RGF0YXNldDox/experiments\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDox/compare?experimentId=RXhwZXJpbWVudDoy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting agent with messages: What was the most popular product SKU?\n",
      "Running agent with messages: [{'role': 'user', 'content': 'What was the most popular product SKU?'}]\n",
      "Added system prompt to messages\n",
      "Starting router\n",
      "Received response with tool calls: True\n",
      "Processing tool calls\n",
      "Starting router\n",
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n",
      "Starting agent with messages: What was the total revenue across all stores?\n",
      "Running agent with messages: [{'role': 'user', 'content': 'What was the total revenue across all stores?'}]\n",
      "Added system prompt to messages\n",
      "Starting router\n",
      "Received response with tool calls: True\n",
      "Processing tool calls\n",
      "Starting router\n",
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n",
      "Starting agent with messages: Which store had the highest sales volume?\n",
      "Running agent with messages: [{'role': 'user', 'content': 'Which store had the highest sales volume?'}]\n",
      "Added system prompt to messages\n",
      "Starting router\n",
      "Received response with tool calls: True\n",
      "Processing tool calls\n",
      "Starting router\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n",
      "Starting agent with messages: Create a bar chart showing total sales by store\n",
      "Running agent with messages: [{'role': 'user', 'content': 'Create a bar chart showing total sales by store'}]\n",
      "Added system prompt to messages\n",
      "Starting router\n",
      "Received response with tool calls: True\n",
      "Processing tool calls\n",
      "Starting router\n",
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n",
      "Starting agent with messages: What percentage of items were sold on promotion?\n",
      "Running agent with messages: [{'role': 'user', 'content': 'What percentage of items were sold on promotion?'}]\n",
      "Added system prompt to messages\n",
      "Starting router\n",
      "Received response with tool calls: True\n",
      "Processing tool calls\n",
      "Starting router\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n",
      "Starting agent with messages: What was the average transaction value?\n",
      "Running agent with messages: [{'role': 'user', 'content': 'What was the average transaction value?'}]\n",
      "Added system prompt to messages\n",
      "Starting router\n",
      "Received response with tool calls: True\n",
      "Processing tool calls\n",
      "Starting router\n",
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n",
      "Starting agent with messages: Create a line chart showing sales in 2021\n",
      "Running agent with messages: [{'role': 'user', 'content': 'Create a line chart showing sales in 2021'}]\n",
      "Added system prompt to messages\n",
      "Starting router\n",
      "Received response with tool calls: True\n",
      "Processing tool calls\n",
      "Starting router\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 (100.0%) | ‚è≥ 00:23<00:00 |  3.42s/it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.88s/it\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:05<00:00 |  5.78s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:05<00:00 |  5.93s/it\n",
      "/Users/priyanka./Documents/ai-agents/venv/lib/python3.11/site-packages/phoenix/experiments/evaluators/utils.py:229: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  return EvaluationResult(score=float(result))\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.18s/it\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:05<00:00 |  5.66s/it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:08<00:00 |  8.75s/it\n",
      "/Users/priyanka./Documents/ai-agents/venv/lib/python3.11/site-packages/phoenix/experiments/evaluators/utils.py:229: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  return EvaluationResult(score=float(result))\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.40s/it\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:04<00:00 |  4.66s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:04<00:00 |  4.67s/it\n",
      "/Users/priyanka./Documents/ai-agents/venv/lib/python3.11/site-packages/phoenix/experiments/evaluators/utils.py:229: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  return EvaluationResult(score=float(result))\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.48s/it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:04<00:00 |  4.79s/it\n",
      "/Users/priyanka./Documents/ai-agents/venv/lib/python3.11/site-packages/phoenix/experiments/evaluators/utils.py:229: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  return EvaluationResult(score=float(result))\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:06<00:00 |  6.01s/it\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.32s/it\n",
      "/Users/priyanka./Documents/ai-agents/venv/lib/python3.11/site-packages/phoenix/experiments/evaluators/utils.py:229: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  return EvaluationResult(score=float(result))\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:03<00:00 |  3.76s/it\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:05<00:00 |  5.96s/it\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:06<00:00 |  6.39s/it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:06<00:00 |  6.95s/it\n",
      "/Users/priyanka./Documents/ai-agents/venv/lib/python3.11/site-packages/phoenix/experiments/evaluators/utils.py:229: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  return EvaluationResult(score=float(result))\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:02<00:00 |  2.72s/it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:04<00:00 |  4.54s/it\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:04<00:00 |  4.89s/it\n",
      "\n",
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:04<00:00 |  4.19s/it\n",
      "/Users/priyanka./Documents/ai-agents/venv/lib/python3.11/site-packages/phoenix/experiments/evaluators/utils.py:229: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  return EvaluationResult(score=float(result))\n",
      "\n",
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 (100.0%) | ‚è≥ 00:48<00:00 |  1.40s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDox/compare?experimentId=RXhwZXJpbWVudDoy\n",
      "\n",
      "Experiment Summary (07/10/25 08:20 PM +0530)\n",
      "--------------------------------------------\n",
      "                     evaluator  n  n_scores  avg_score  n_labels top_2_labels\n",
      "0             code_is_runnable  7         7   1.000000         7  {'True': 7}\n",
      "1             evaluate_clarity  7         7   0.428571         0         None\n",
      "2  evaluate_entity_correctness  7         7   1.000000         0         None\n",
      "3          evaluate_sql_result  7         7   1.000000         7  {'True': 7}\n",
      "4        function_calling_eval  7         7   0.000000         0         None\n",
      "\n",
      "Tasks Summary (07/10/25 08:19 PM +0530)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0           7       7         0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = run_experiment(dataset,\n",
    "                            run_agent_task,\n",
    "                            evaluators=[function_calling_eval, \n",
    "                                        evaluate_sql_result, \n",
    "                                        evaluate_clarity, \n",
    "                                        evaluate_entity_correctness,\n",
    "                                        code_is_runnable],\n",
    "                            experiment_name=\"Overall Experiment v2\",\n",
    "                            experiment_description=\"Evaluating the overall experiment, with changes to sql prompt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
